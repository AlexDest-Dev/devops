# Lab 13
## Commands
```
$ kubectl get po,sts,svc,pvc
NAME                READY   STATUS    RESTARTS   AGE
pod/moscow-time-0   1/1     Running   0          8m54s
pod/moscow-time-1   1/1     Running   0          8m54s

NAME                           READY   AGE
statefulset.apps/moscow-time   2/2     8m54s

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes    ClusterIP   10.96.0.1       <none>        443/TCP    65m
service/moscow-time   ClusterIP   10.100.82.200   <none>        5000/TCP   8m54s

NAME                                                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/moscow-time-state-volume-moscow-time-0   Bound    pvc-cf87b02f-5306-4ecd-9163-3a515b688b99   128Mi      RWO            standard       8m54s
persistentvolumeclaim/moscow-time-state-volume-moscow-time-1   Bound    pvc-fa66ad7f-9eb5-4f06-a2f4-449b5d66d6bb   128Mi      RWO            standard       8m54s
```

```
$ kubectl exec moscow-time-1 -- cat ./persistent/visits.json
{"list": ["0:38:43", "0:38:53", "0:39:3", "0:39:3", "0:39:10", "0:39:12", "0:39:13", "0:39:13", "0:39:13", "0:39:22", "0:39:23", "0:39:23", "0:39:33", "0:39:43", "0:39:53", "0:39:53", "0:40:3", "0:40:3", "0:40:13", "0:40:13", "0:40:23", "0:40:33", "0:40:43", "0:41:17", "0:41:25", "0:41:25", "0:41:35", "0:41:35"], "total": 28}

$ kubectl exec moscow-time-1 -- cat ./persistent/visits.json
{"list": ["0:38:43", "0:38:53", "0:39:3", "0:39:3", "0:39:10", "0:39:12", "0:39:13", "0:39:13", "0:39:13", "0:39:22", "0:39:23", "0:39:23", "0:39:33", "0:39:43", "0:39:53", "0:39:53", "0:40:3", "0:40:3", "0:40:13", "0:40:13", "0:40:23", "0:40:33", "0:40:43", "0:41:17", "0:41:25", "0:41:25", "0:41:35", "0:41:35", "0:41:45", "0:41:45", "0:41:49", "0:41:51", "0:41:51", "0:41:55", "0:42:5"], "total": 35}
```

## Visit logs result
We have in first replica 6 visits and in second replica 4 visits - the difference here because of load balancer, which distribute load from one replica to another. Also each replica independents from other replicas and has its own storage for data.

## Ordering
For our application and replicas ordering unnecesary because each replica is independent from other replicas and if any of them fail, other will work.

Way for parallel executing - add `podManagementPolicy: "Parallel"` to configuration